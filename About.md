# Bridging Theory to Practice with AI

[David Klaasen](https://www.linkedin.com/in/david-k-at-work/)
Director of Learning Design | Learning & Development | Learning Solution Architecture | Enterprise Learning | Portfolio Management | Programme Management
July 4, 2025

When I first read about [MIT’s Learning Engineering Evidence and Decision (LEED) Tracker](https://edtechbooks.org/jaid_13_2/why_did_we_do_that_a_systematic_approach_to_tracking_decisions_in_the_design_and_iteration_of_learning_experiences), I thought it was a good idea, but struggled to visualise its use in real-world contexts beyond on-campus teams. So, like any eager designer would do, I decided to build one myself for learning professionals working outside traditional academic settings.

I created [this AI-powered, framework-driven, interactive tool](https://interactive-leed-tracker.lovable.app/) as both a practitioner's solution and a community resource. It transforms what's usually administrative overhead (i.e. documenting why we made specific design choices) into strategic advantage. Again, if it serves you and your team, amazing! If not, you might find value in seeing an approach to bridging the gap from theory to practice.

The Discovery tab shows how teams can systematically capture their initial research and stakeholder insights, creating a clear foundation for subsequent design decisions.

## The Challenge

If you’ve worked with learning teams across different contexts, you might have encountered these specific failure modes that undermine evidence-based practice:

**Invisible decision-making:** Critical design choices live in people's heads, email threads, or hard-to-find chat messages. When someone asks, "Why did we do it this way?", the reason is long-forgotten. This siloes knowledge and makes quality assurance near-impossible, especially in organisations where mobility is common or turnover is high.

**Cognitive overload during active design:** Teams managing lots of moving parts often give up on systematic approaches when mental load peaks. The same complexity that demands structured thinking makes structured thinking harder to sustain.

**Weak evidence linkage:** It's surprisingly easy to skip the "prove it" step when deadlines loom. Design decisions that felt obviously right in the moment often lack traceable connections to research or data, making post-project evaluation and improvement difficult.

**Poor visibility for oversight:** Managers and peers struggle to review progress, spot risks, or identify exemplary practice without diving deep into project files. Quality assurance becomes detective work rather than systematic monitoring.

The issue isn't lack of knowledge or commitment. It’s a culture of busyness. These are predictable organisational challenges that we can address directly through tools that embed clear thinking. So, to build an effective solution, I needed a solid theoretical foundation. And that's where the GOLD Framework comes in.

## Introducing the GOLD Framework

Before sharing the solution, it's worth explaining what informs the LEED Tracker: the Guidelines for Optimal Learning Design (GOLD) Framework.

Rather than overwhelming users with all seven principles at once, the reference guide presents GOLD Framework elements contextually, reducing cognitive load whilst maintaining comprehensive coverage.

GOLD isn't another acronym forced to fit convenient letters. It's an umbrella term for the research-backed principles that speak to how people actually learn, what they bring into learning environments, and how those environments can be optimised accordingly.

The framework consists of seven complementary approaches that are relevant, not just in the design phase, but throughout the learning development lifecycle:

**Cognitive Load Theory** guides how we manage the mental effort required for learning, ensuring we don't overwhelm working memory whilst building long-term understanding.

**Rosenshine's Principles of Instruction** provides systematic scaffolding strategies that support learners through the acquisition of new knowledge and skills.

**Multimedia Learning Theory** helps us design visual and auditory elements that genuinely enhance rather than compete with core learning processes.

**Retrieval Practice** focuses on strengthening memory consolidation through strategic recall opportunities rather than passive review.

**Desirable Difficulties** introduces productive challenges that deepen learning without creating unnecessary frustration.

**Trauma-Informed Pedagogy** ensures learners feel psychologically safe, emotionally respected, and academically empowered.

**Universal Design for Learning** creates inclusive approaches that work for varied learning needs from the outset rather than requiring retrofitted accommodations.

Together, these principles form a solid foundation for evidence-based learning design; one that addresses not just cognitive processes, but the broader complexity of learning contexts.

These evidence quality standards help teams distinguish between robust research, expert opinion, and anecdotal evidence to ensure that design decisions rest on firm foundations.

**Note:** The GOLD Framework is the evidence base that I created for the [Design Lab custom GPT](https://chatgpt.com/g/g-6846a096e8788191b57a4c513a82db7d-design-lab). Since your organisation is likely to have an existing pedagogical model, consider remixing the LEED tracker in Lovable and swapping out GOLD with your own approach. That said, if your team is in the early stages of its learning design maturity, feel free to use GOLD as a launchpad.

## Building the Solution

The LEED Tracker repositions the GOLD Framework as a decision-support system that directly addresses the above-mentioned failure modes.

**Externalising tacit knowledge:** The tool forces every significant design decision to be logged against a consistent outline: phase, rationale, evidence, theoretical backing. This creates what cognitive scientists call environmental support, offloading details from working memory so teams can reason at higher levels about design strategy.

Each decision card captures not just what was decided, but why it was decided and what alternatives were considered. This transforms invisible reasoning into visible, reviewable rationale.The structured input fields ensure teams address critical decision elements systematically: evidence sources, alternative options, potential risks, AI considerations, and evaluation criteria.

**Managing cognitive load:** Rather than presenting all theories simultaneously, the interface breaks workflow into phase-specific decision cards with transition checklists. This chunks complex tasks into manageable elements, reducing unnecessary mental effort whilst preserving focus on core learning activities.

**Enforcing evidence links:** Users must select a GOLD principle and attach concrete evidence (e.g. research citations, data, or expert judgement) before forms can be saved. This puts Rosenshine's principles into practice. Teams must ground their design decisions in solid evidence, just as effective instruction grounds learning in clear foundations.

**Creating visibility:** Real-time dashboards reveal theory usage patterns, evidence gaps, and phase completion rates. Quality assurance becomes systematic monitoring rather than detective work, increasing the visibility that's identified as essential for adopting new practices.

**Enabling evaluation:** Being able to export captured data allows evaluators to correlate design decisions, evidence quality, and learning outcomes. Good evaluation requires good documentation, and this tracker provides the audit trail that makes continuous improvement possible.

## Design Principles in Action

**Progressive Disclosure:** The full GOLD Framework remains available, but contextually rather than overwhelmingly. Teams access the specific principles relevant to their current decision point without navigating unnecessary complexity.

**Visual Progress Tracking:** Rather than administrative reporting, teams get diagnostic feedback on their decision patterns across project phases. This builds metacognitive awareness of their evidence-based practice development.

**Export Flexibility:** Different stakeholders require different formats. The tool generates PDFs for presentations, Excel files for analysis, Word documents for collaboration. In other words, it adapts to organisational workflows rather than imposing new ones.

Export flexibility means the same decision data can serve multiple stakeholders: PDFs for presentations, Excel for analysis, Word for collaborative review.The PDF export creates a professional document trail that captures months of decision-making in a format suitable for stakeholder review, compliance requirements, or project handovers.

**Real-Time Application:** Teams can capture rationale and alternatives during decision-making moments, not as separate documentation phases. This should remove the friction between thinking and recording.

## Investing Time Upfront for Better Outcomes Later

Using Lovable's development platform, I translated the LEED template into an interactive web application. The build process revealed how interface design itself needs to follow learning science principles. In that way, the tracker is self-reinforcing; it applies the GOLD Framework principles to help teams apply GOLD Framework principles. For instance, it uses cognitive load management to support the application of cognitive load management.

However, any tool is only as effective as its adoption. Teams need time to integrate structured decision-making into their existing workflows, and managers need to see the value in investing that time upfront for better outcomes later. This principle seems common to many emerging AI use cases where what you put in significantly determines the quality of what you get out.

## Going Forward

Building the LEED Tracker reinforced my belief that our field needs more interactive frameworks, not more comprehensive ones. We have reams of research, but what we often lack are tools that make applying this research useful and sustainable under real-world conditions. So this has been an exercise in bridging theory to practice.

If you're facing similar challenges with maintaining evidence-based practice under project pressures, I'd welcome your feedback on this approach. The tracker is available as a working prototype, and real-world testing will help refine it further. My sense is that sometimes the most valuable tools emerge not from solving new problems, but from making existing good practice easier to sustain consistently.
